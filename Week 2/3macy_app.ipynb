{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting fake_useragent\n",
      "  Using cached fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gitpod/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/gitpod/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
      "Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, fake_useragent, tzdata, numpy, pandas\n",
      "Successfully installed fake_useragent-1.5.1 numpy-2.1.3 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import concurrent.futures\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_brands = []\n",
    "product_names = []\n",
    "current_prices = []\n",
    "old_prices = []\n",
    "ratings = []\n",
    "product_urls = []\n",
    "\n",
    "your_url = input(str('Please paste macy link with id=number at the end like https://www.macys.com/shop/handbags-accessories?id=26846'))\n",
    "\n",
    "def scrape_macy(url):\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    headers = {'User-Agent': userAgent}\n",
    "    page = requests.get(url, headers = headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    sales_box = soup.find_all('div', class_='product-thumbnail-container')\n",
    "\n",
    "\n",
    "    for box in sales_box:\n",
    "        #product_brand\n",
    "        if box.find('div', class_ = 'product-brand medium') is not None:\n",
    "            product_brand = box.find('div', class_ = 'product-brand medium').text.lstrip()\n",
    "            product_brands.append(product_brand)\n",
    "        else:\n",
    "            product_brands.append('None')\n",
    "\n",
    "                #product_name\n",
    "        if box.find('div', class_ = \"product-name medium\") is not None:\n",
    "            product_name = box.find('div', class_ = \"product-name medium\").text.lstrip() \n",
    "            product_names.append(product_name)\n",
    "        else:\n",
    "            product_names.append('None')  \n",
    "\n",
    "\n",
    "            #price\n",
    "        if box.find('span', class_ = \"discount price-reg\") is not None:\n",
    "            current_price = box.find('span', class_ = \"discount price-reg\").text.strip() \n",
    "            current_prices.append(current_price)\n",
    "        elif box.find('span', class_ = \"price-reg\") is not None:\n",
    "            current_price = box.find('span', class_ = \"price-reg\").text.strip()\n",
    "            current_prices.append(current_price)\n",
    "        else:\n",
    "            current_prices.append('None')  \n",
    "\n",
    "                #old price \n",
    "        if box.find('span', class_ = \"price-strike\") is not None:\n",
    "            old_price = box.find('span', class_ = \"price-strike\").text\n",
    "            old_prices.append(old_price)\n",
    "        else:\n",
    "            old_prices.append('None')\n",
    "\n",
    "            #rating\n",
    "        if box.select_one('div.rating > div > fieldset') is not None:\n",
    "            rating = box.select_one('div.rating > div > fieldset')\n",
    "            rating = rating['aria-label']\n",
    "            ratings.append(rating)\n",
    "        else:\n",
    "            ratings.append('None')\n",
    "\n",
    "\n",
    "        if box.find('a', attrs={\"href\": True}) is not None:\n",
    "            product_url = box.find('a', attrs={\"href\": True}) \n",
    "            product_url = product_url['href']\n",
    "            product_urls.append('https://www.macys.com'+product_url)\n",
    "        else:\n",
    "            product_urls.append('None')\n",
    "\n",
    "    if soup.find('a', class_ = 'pagination-next') is None:\n",
    "        a = 1\n",
    "        b = 'e'\n",
    "        a+b\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "base_url, query_string = your_url.split('?', 1)\n",
    "index = 1\n",
    "while True:\n",
    "    try: \n",
    "        page = '/Pageindex/'+ '{}?{}'.format(index,query_string)\n",
    "        index += 1\n",
    "        new_url = base_url+page\n",
    "        scrape_macy(new_url)\n",
    "        print(new_url)\n",
    "        print(len(product_brands))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Product Brand': product_brands,\n",
    "    'Product Name': product_names,\n",
    "    'Current Price': current_prices,\n",
    "    'Old Price': old_prices,\n",
    "    'Rating': ratings,\n",
    "    'Product URL': product_urls\n",
    "})\n",
    "\n",
    "\n",
    "product_description = []\n",
    "link = []\n",
    "\n",
    "def scrape_inner_page(url):\n",
    "\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    headers = {'User-Agent': userAgent}\n",
    "    new_page = requests.get(url, headers = headers)\n",
    "    new_soup = BeautifulSoup(new_page.content, \"html.parser\")\n",
    "\n",
    "    if new_soup.find('li', attrs={'data-auto': 'product-summary-section'}) is not None:\n",
    "        details = new_soup.find('li', attrs={'data-auto': 'product-summary-section'})\n",
    "        product_description.append(details.text)\n",
    "    else:\n",
    "        product_description.append('None')\n",
    "\n",
    "    link.append(url)\n",
    "    print(url)\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    executor.map(scrape_inner_page,product_urls)\n",
    "\n",
    "\n",
    "data_inner = pd.DataFrame({\n",
    "    'Product Description': product_description,\n",
    "    'Product URL': link\n",
    "})\n",
    "\n",
    "result = pd.merge(data, data_inner, left_on='Product URL', right_on='Product URL', how='inner')\n",
    "result.to_csv(f'{base_url.split('/')[-1]}.csv')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
